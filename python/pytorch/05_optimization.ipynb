{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This comes from previous notebooks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Hyperparameters are adjustable parameters that let you control the model optimization process.\n",
    "\n",
    "For example:\n",
    "\n",
    "- `learning_rate` (how much to update models parameters at each batch/epoch)\n",
    "- `batch_size` (number of data samples propagated through the network before updating parameters)\n",
    "- `epochs` (number of times to iterate over the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization loop\n",
    "\n",
    "Each iteration of the optimizatoin loop is called an _epoch_.\n",
    "\n",
    "- **Train loop**: iterate over the training dataset and try to converge to optimal parameters.\n",
    "- **Validation/test loop**: iterate over the test dataset to check if model performance is improving.\n",
    "\n",
    "The _loss function_ measures the degree of dissimilarity between predictions and real values.\n",
    "\n",
    "- `MSELoss` for regression\n",
    "- `NLLLoss` for classification\n",
    "- `CrossEntropyLoss` combines `NLLLoss` and `LogSoftmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Optimization involves adjusting the model parameters to reduce prediction errors in each training step.\n",
    "\n",
    "Optimization algorithms define how this process is performed (i.e. Stochastic Gradient Descent, ADAM, RMSProp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimizatio happens in three steps:\n",
    "\n",
    "1. `optimizer.zero_grad()` is called to reset the gradients of model parameters\n",
    "2. backpropagation of the prediction error after calling `loss.backward()`\n",
    "3. `optimizer.step()` adjust the parameters by the gradients collected in the backward pass\n",
    "\n",
    "### Training loop\n",
    "\n",
    "Let's see the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    model.train() # set the model to training mode. Important for batch normalization and dropout\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # PREDICT AND GET LOSS\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the test loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval() # set to evaluation mode\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0,0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f'Test error: \\n Accuracy: {(100*correct) :>0.1f}%, Avg loss: {test_loss:>8f}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model with `torch.no_grad()` ensures that no gradients are computed during test mode. Also serves to reduce unnecessary gradient computations and memory usage.\n",
    "\n",
    "Now let's train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.312810 [   64/60000]\n",
      "loss: 2.299899 [ 6464/60000]\n",
      "loss: 2.275367 [12864/60000]\n",
      "loss: 2.265247 [19264/60000]\n",
      "loss: 2.253852 [25664/60000]\n",
      "loss: 2.220118 [32064/60000]\n",
      "loss: 2.229505 [38464/60000]\n",
      "loss: 2.195182 [44864/60000]\n",
      "loss: 2.195304 [51264/60000]\n",
      "loss: 2.151009 [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 43.4%, Avg loss: 2.152643\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.165602 [   64/60000]\n",
      "loss: 2.156515 [ 6464/60000]\n",
      "loss: 2.089532 [12864/60000]\n",
      "loss: 2.107549 [19264/60000]\n",
      "loss: 2.061419 [25664/60000]\n",
      "loss: 1.990726 [32064/60000]\n",
      "loss: 2.016929 [38464/60000]\n",
      "loss: 1.935583 [44864/60000]\n",
      "loss: 1.949271 [51264/60000]\n",
      "loss: 1.859064 [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 55.2%, Avg loss: 1.867659\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.902026 [   64/60000]\n",
      "loss: 1.873279 [ 6464/60000]\n",
      "loss: 1.746281 [12864/60000]\n",
      "loss: 1.794640 [19264/60000]\n",
      "loss: 1.697418 [25664/60000]\n",
      "loss: 1.635470 [32064/60000]\n",
      "loss: 1.659032 [38464/60000]\n",
      "loss: 1.563401 [44864/60000]\n",
      "loss: 1.600969 [51264/60000]\n",
      "loss: 1.482060 [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 62.0%, Avg loss: 1.508873\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.574551 [   64/60000]\n",
      "loss: 1.542146 [ 6464/60000]\n",
      "loss: 1.390078 [12864/60000]\n",
      "loss: 1.468554 [19264/60000]\n",
      "loss: 1.365718 [25664/60000]\n",
      "loss: 1.347575 [32064/60000]\n",
      "loss: 1.361962 [38464/60000]\n",
      "loss: 1.290681 [44864/60000]\n",
      "loss: 1.335880 [51264/60000]\n",
      "loss: 1.224904 [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 64.0%, Avg loss: 1.255669\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.328794 [   64/60000]\n",
      "loss: 1.312858 [ 6464/60000]\n",
      "loss: 1.146390 [12864/60000]\n",
      "loss: 1.253931 [19264/60000]\n",
      "loss: 1.141842 [25664/60000]\n",
      "loss: 1.155191 [32064/60000]\n",
      "loss: 1.173758 [38464/60000]\n",
      "loss: 1.115511 [44864/60000]\n",
      "loss: 1.164185 [51264/60000]\n",
      "loss: 1.068933 [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 65.4%, Avg loss: 1.092735\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.159585 [   64/60000]\n",
      "loss: 1.163417 [ 6464/60000]\n",
      "loss: 0.981988 [12864/60000]\n",
      "loss: 1.114131 [19264/60000]\n",
      "loss: 0.997645 [25664/60000]\n",
      "loss: 1.021016 [32064/60000]\n",
      "loss: 1.053502 [38464/60000]\n",
      "loss: 0.999388 [44864/60000]\n",
      "loss: 1.047171 [51264/60000]\n",
      "loss: 0.968767 [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 66.3%, Avg loss: 0.984374\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.038377 [   64/60000]\n",
      "loss: 1.064099 [ 6464/60000]\n",
      "loss: 0.866619 [12864/60000]\n",
      "loss: 1.018760 [19264/60000]\n",
      "loss: 0.904179 [25664/60000]\n",
      "loss: 0.924601 [32064/60000]\n",
      "loss: 0.973763 [38464/60000]\n",
      "loss: 0.921558 [44864/60000]\n",
      "loss: 0.963911 [51264/60000]\n",
      "loss: 0.901317 [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 67.5%, Avg loss: 0.909541\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.948195 [   64/60000]\n",
      "loss: 0.994043 [ 6464/60000]\n",
      "loss: 0.783203 [12864/60000]\n",
      "loss: 0.950690 [19264/60000]\n",
      "loss: 0.841780 [25664/60000]\n",
      "loss: 0.853655 [32064/60000]\n",
      "loss: 0.917355 [38464/60000]\n",
      "loss: 0.868837 [44864/60000]\n",
      "loss: 0.903897 [51264/60000]\n",
      "loss: 0.853382 [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 68.5%, Avg loss: 0.855799\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.879081 [   64/60000]\n",
      "loss: 0.941590 [ 6464/60000]\n",
      "loss: 0.721084 [12864/60000]\n",
      "loss: 0.900209 [19264/60000]\n",
      "loss: 0.798054 [25664/60000]\n",
      "loss: 0.800775 [32064/60000]\n",
      "loss: 0.874944 [38464/60000]\n",
      "loss: 0.832089 [44864/60000]\n",
      "loss: 0.859574 [51264/60000]\n",
      "loss: 0.817261 [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 69.7%, Avg loss: 0.815533\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.824507 [   64/60000]\n",
      "loss: 0.899717 [ 6464/60000]\n",
      "loss: 0.673231 [12864/60000]\n",
      "loss: 0.861488 [19264/60000]\n",
      "loss: 0.765736 [25664/60000]\n",
      "loss: 0.760406 [32064/60000]\n",
      "loss: 0.841185 [38464/60000]\n",
      "loss: 0.805139 [44864/60000]\n",
      "loss: 0.825682 [51264/60000]\n",
      "loss: 0.788571 [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 71.0%, Avg loss: 0.783923\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
